# Observoor Example Configuration
#
# Copy this file and customize for your environment.
# Run with: sudo ./observoor --config config.yaml

log_level: info

# Metadata for exported data (used in ClickHouse meta_* columns)
meta_client_name: "my-observoor-instance"
meta_network_name: "mainnet"

# Beacon node connection (required)
beacon:
  endpoint: http://localhost:5052
  timeout: 10s

# Process discovery (optional - defaults to all known Ethereum clients)
# If omitted, searches for: geth, reth, besu, nethermind, erigon,
# lighthouse, prysm, beacon-chain, validator, teku, lodestar, nimbus
pid:
  # Option 1: Discover by process name (scans /proc)
  # process_names:
  #   - geth
  #   - lighthouse
  # Option 2: Discover by cgroup path
  # cgroup_path: /sys/fs/cgroup/ethereum.slice

# BPF ring buffer size in bytes (default: 4MB)
ring_buffer_size: 4194304

# How often to poll beacon node for sync status (default: 30s)
sync_poll_interval: 30s

# Data export sinks
sinks:
  # Aggregated metrics - configurable resolution aggregation
  aggregated:
    enabled: true
    resolution:
      # Aggregation window duration (50ms, 100ms, 500ms, 1s, 5s, 1m)
      interval: 1s
      # Reset aggregation at slot boundaries
      slot_aligned: true
      # How often to write sync state (default: 12s)
      sync_state_poll_interval: 12s
      # Optional per-metric slower intervals.
      # - Each override interval must be greater than `interval`.
      # - Each override interval must be an exact multiple of `interval`.
      # - Metric names must be unique across all overrides.
      # overrides:
      #   - metrics: [syscall_futex, syscall_mmap, sched_runqueue, mem_reclaim, mem_compaction]
      #     interval: 5s
      #   - metrics: [page_fault_major, page_fault_minor, swap_in, swap_out, oom_kill, fd_open, fd_close, process_exit, tcp_state_change]
      #     interval: 10s
    dimensions:
      network:
        # Include port label in network metrics (e.g., el_p2p_tcp, cl_p2p_udp)
        include_port: true
        # Include TX/RX direction in network metrics
        include_direction: true
      disk:
        # Include device ID in disk metrics
        include_device: true
        # Include read/write breakdown in disk metrics
        include_rw: true
    sampling:
      # Kernel-side per-event sampling (drops before ring buffer export).
      # mode: none | probability | nth
      # - probability: rate is keep probability (0 < rate <= 1)
      # - nth: rate must be exactly 1/N (e.g. 0.5, 0.25, 0.1)
      # If network.include_direction is false, net_tx and net_rx rules must match.
      default:
        mode: none
        rate: 1.0
      # events:
      #   syscall_futex:
      #     mode: nth
      #     rate: 0.1     # keep every 10th event
      #   net_tx:
      #     mode: probability
      #     rate: 0.25
      #   net_rx:
      #     mode: probability
      #     rate: 0.25
    clickhouse:
      enabled: true
      endpoint: clickhouse:9000
      database: observoor
      # Base table prefix (default: aggregated_metrics)
      table: aggregated_metrics
      batch_size: 10000
      flush_interval: 1s
      # Optional ClickHouse auth
      # username: default
      # password: changeme
      migrations:
        enabled: true
      # sampling_mode/sampling_rate columns reflect effective eBPF sampling config.
    # Optional HTTP export (e.g., to Vector)
    # http:
    #   enabled: true
    #   address: http://vector:8080/aggregated
    #   compression: gzip      # none, gzip, zstd, zlib, snappy
    #   batch_size: 512
    #   batch_timeout: 5s
    #   export_timeout: 30s
    #   max_queue_size: 8192
    #   workers: 1
    #   keep_alive: true
    #   headers:
    #     Authorization: "Bearer token"

# Prometheus metrics endpoint
health:
  addr: ":9090"
